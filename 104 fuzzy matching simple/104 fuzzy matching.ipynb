{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples of fuzzy matching \n",
    "\n",
    "updated \n",
    "21/2/2021\n",
    "These example routines use a special library in Python called fuzzywuzzy which implements various strategies for ranking similarity. Absolutely use it. There are other basic libraries but then you need to program the logic on top to make the best choices.\n",
    "\n",
    "Note that this approach works for a relatively low number of records. If you are talking several 100K records things will become exponential and you may not be able to really run these. There are some approaches for large scale comparisons in a separate recipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/mobile/Containers/Data/Application/E26F1E42-C4E9-412E-98C9-C0D10CAA235E/Documents/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "from datetime import datetime\n",
    "#import Levenshtein\n",
    "#Note: apparently installing the python-Levenshtein module alongside fuzzywuzzy can increase performance\n",
    "\n",
    "#we need this for the alternative method of self matching, ie fuzzy duplicates \n",
    "from itertools import combinations \n",
    "\n",
    "\n",
    "\n",
    "df_short= pd.read_excel(\"./test_data/data_short.xlsx\")\n",
    "df_long=pd.read_excel(\"./test_data/data_long.xlsx\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspired by  \n",
    "\n",
    "https://www.datacamp.com/community/tutorials/fuzzy-string-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Levenshtein distance is a metric to measure how apart are two sequences of words. In other words, it measures the minimum number of edits that you need to do to change a one-word sequence into the other. These edits can be insertions, deletions or substitutions. This metric was named after Vladimir Levenshtein, who originally considered it in 1965."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95\n"
     ]
    }
   ],
   "source": [
    "Str1 = \"Apple Inc.\"\n",
    "Str2 = \"apple Inc\"\n",
    "Ratio = fuzz.ratio(Str1.lower(),Str2.lower())\n",
    "print(Ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n",
      "73\n",
      "64\n"
     ]
    }
   ],
   "source": [
    "Str1 = \"Los Angeles Lakers\"\n",
    "Str2 = \"L.A. Lakers\"\n",
    "Ratio = fuzz.ratio(Str1.lower(),Str2.lower())\n",
    "Partial_Ratio = fuzz.partial_ratio(Str1.lower(),Str2.lower())\n",
    "Token_Sort_Ratio = fuzz.token_sort_ratio(Str1.lower(),Str2.lower())\n",
    "print(Ratio)\n",
    "print(Partial_Ratio)\n",
    "print(Token_Sort_Ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fuzz.partial_ratio() is capable of detecting that both strings are referring to the Lakers. Thus, it yields 100% similarity. The way this works is by using an \"optimal partial\" logic. In other words, if the short string has length k and the longer string has the length m, then the algorithm seeks the score of the best matching length-k substring.\n",
    "\n",
    "Nevertheless, this approach is not foolproof. What happens when the strings comparison the same, but they are in a different order?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n",
      "74\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "Str1 = \"united states v. nixon\"\n",
    "Str2 = \"Nixon v. United States\"\n",
    "Ratio = fuzz.ratio(Str1.lower(),Str2.lower())\n",
    "Partial_Ratio = fuzz.partial_ratio(Str1.lower(),Str2.lower())\n",
    "Token_Sort_Ratio = fuzz.token_sort_ratio(Str1,Str2)\n",
    "print(Ratio)\n",
    "print(Partial_Ratio)\n",
    "print(Token_Sort_Ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fuzz.token functions have an important advantage over ratio and partial_ratio. They tokenize the strings and preprocess them by turning them to lower case and getting rid of punctuation. In the case of fuzz.token_sort_ratio(), the string tokens get sorted alphabetically and then joined together. After that, a simple fuzz.ratio() is applied to obtain the similarity percentage. This allows cases such as court cases in this example to be marked as being the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still, what happens if these two strings are of widely differing lengths? Thats where fuzz.token_set_ratio() comes in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n",
      "77\n",
      "58\n",
      "95\n"
     ]
    }
   ],
   "source": [
    "Str1 = \"The supreme court case of Nixon vs The United States\"\n",
    "Str2 = \"Nixon v. United States\"\n",
    "Ratio = fuzz.ratio(Str1.lower(),Str2.lower())\n",
    "Partial_Ratio = fuzz.partial_ratio(Str1.lower(),Str2.lower())\n",
    "Token_Sort_Ratio = fuzz.token_sort_ratio(Str1,Str2)\n",
    "Token_Set_Ratio = fuzz.token_set_ratio(Str1,Str2)\n",
    "print(Ratio)\n",
    "print(Partial_Ratio)\n",
    "print(Token_Sort_Ratio)\n",
    "print(Token_Set_Ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fuzz.token_set_ratio() takes a more flexible approach than fuzz.token_sort_ratio(). Instead of just tokenizing the strings, sorting and then pasting the tokens back together, token_set_ratio performs a set operation that takes out the common tokens (the intersection) and then makes fuzz.ratio() pairwise comparisons between the following new strings:\n",
    "\n",
    "s1 = Sorted_tokens_in_intersection  \n",
    "s2 = Sorted_tokens_in_intersection + sorted_rest_of_str1_tokens  \n",
    "s3 = Sorted_tokens_in_intersection + sorted_rest_of_str2_tokens  \n",
    "The logic behind these comparisons is that since Sorted_tokens_in_intersection is always the same, the score will tend to go up as these words make up a larger chunk of the original strings or the remaining tokens are closer to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['new line', 'inaqui', 'tab entry', 'lucia', 'ryan oneill', 'ana maria', 'john smith 2nd', 'peter drucker', 'emma watson', 'jeff bezos', 'jeff bezos']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>expected</th>\n",
       "      <th>fuzzy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>New\\rLine</td>\n",
       "      <td>new line</td>\n",
       "      <td>new line</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Iñaqui</td>\n",
       "      <td>inaqui</td>\n",
       "      <td>inaqui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tab\\tEntry</td>\n",
       "      <td>tab entry</td>\n",
       "      <td>tab entry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lucía</td>\n",
       "      <td>lucia</td>\n",
       "      <td>lucia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ryan     O'Neill</td>\n",
       "      <td>ryan oneill</td>\n",
       "      <td>ryan oneill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ana-María</td>\n",
       "      <td>ana maria</td>\n",
       "      <td>ana maria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>John Smith 2nd</td>\n",
       "      <td>john smith 2nd</td>\n",
       "      <td>john smith 2nd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Peter＿Drücker</td>\n",
       "      <td>peter drucker</td>\n",
       "      <td>peter drucker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Emma_Watson</td>\n",
       "      <td>emma watson</td>\n",
       "      <td>emma watson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Jeff Bezos</td>\n",
       "      <td>jeff bezos</td>\n",
       "      <td>jeff bezos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>jeff   . Bezos</td>\n",
       "      <td>jeff bezos</td>\n",
       "      <td>jeff bezos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 input        expected           fuzzy\n",
       "0            New\\rLine        new line        new line\n",
       "1             Iñaqui            inaqui          inaqui\n",
       "2           Tab\\tEntry       tab entry       tab entry\n",
       "3                Lucía           lucia           lucia\n",
       "4                                  NaN             NaN\n",
       "5     Ryan     O'Neill     ryan oneill     ryan oneill\n",
       "6            Ana-María       ana maria       ana maria\n",
       "7       John Smith 2nd  john smith 2nd  john smith 2nd\n",
       "8        Peter＿Drücker   peter drucker   peter drucker\n",
       "9          Emma_Watson     emma watson     emma watson\n",
       "10          Jeff Bezos      jeff bezos      jeff bezos\n",
       "11    jeff   . Bezos        jeff bezos      jeff bezos"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_fuzzy_column(df,col_name):\n",
    "    df['fuzzy']=df[col_name].str.lower()\n",
    "    df['fuzzy'].replace(\"\\'\", '',regex= True , inplace=True)\n",
    "    df['fuzzy'].replace(r'\\s', ' ', regex = True, inplace = True)\n",
    "    df['fuzzy'].replace(\"\",np.nan,regex=False, inplace=True)\n",
    "    #This will directly remove accented chars and enie , not replace with the vowel or n   \n",
    "    #df['fuzzy'] = df['fuzzy'].str.encode('ascii', 'ignore').str.decode('ascii')    \n",
    "    #This will also remove accented chars \n",
    "    #from string import printable\n",
    "    #st = set(printable)\n",
    "    #df[\"fuzzy\"] = df[\"fuzzy\"].apply(lambda x: ''.join([\" \" if  i not in  st else i for i in x]))      \n",
    "    #This will retain the characters but standardise for compatibility, there are various libraries that do that too.  \n",
    "    df[\"fuzzy\"]= df[\"fuzzy\"].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "    df[\"fuzzy\"].replace('[^a-z0-9 ]', ' ', regex=True, inplace=True)\n",
    "    df['fuzzy'].replace(' +', ' ', regex=True, inplace=True)\n",
    "    df['fuzzy'] = df['fuzzy'].str.strip()\n",
    "    df.set_index('fuzzy')\n",
    "    return list(df['fuzzy'][~pd.isnull(df.fuzzy)])\n",
    "\n",
    "\n",
    "test_data = [[\"  New\\rLine\",\"new line\"],['Iñaqui  ', 'inaqui'], [\"Tab\\tEntry\", \"tab entry\"], ['Lucía',\"lucia\"], [\"\",\"NaN\"], [\"Ryan     O'Neill\",\"ryan oneill\"],[\"Ana-María\",\"ana maria\"],[\"John Smith 2nd\",\"john smith 2nd\"],[\"Peter\\uFF3FDrücker\",\"peter drucker\"],[\"Emma\\u005FWatson\",\"emma watson\"],[\"Jeff Bezos\",\"jeff bezos\"],[\"  jeff   . Bezos  \",\"jeff bezos\"]]\n",
    "test_df = pd.DataFrame(test_data, columns = ['input', 'expected']) \n",
    "test_list= create_fuzzy_column(test_df,'input')\n",
    "print(test_list)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of a routine of checking one file against another"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the fuzzy matching already ignores cases and special chars, it perhaps convenient to do a bit of cleanup in advance. This is because when you pick the unique values via \"set\" command and when you queries for potential matches afterwards, you would have a cleaner and smaller set of data. Possibly worth also removing other artifacts like aphostrophes and double spaces, trimming trailing spaces etc.\n",
    "\n",
    "Also note that using a generic fuzzy field makes the rest of the routine more reusable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 15)\n",
      "1983\n"
     ]
    }
   ],
   "source": [
    "primary_list=list(set(create_fuzzy_column(df_long,\"full_name\"))) \n",
    "#we do set to remove duplicates and reduce the comparisons, we will pick all the exact duplicates\n",
    "#anyway when we reconstruct the hits\n",
    "print(df_long.shape)\n",
    "print(len(primary_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 15)\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "secondary_list= list(set(create_fuzzy_column(df_short,\"full_name\")))\n",
    "print(df_short.shape)\n",
    "print(len(secondary_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = []\n",
    "for item in primary_list:\n",
    "    if type(item) != str: # Checking base case. \n",
    "        pass\n",
    "    else:\n",
    "        match = process.extractOne(item, secondary_list, scorer=fuzz.token_sort_ratio) #Returns tuple of best match and percent fit.\n",
    "        matches.append([item,match[0],match[1]])       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we filter by some threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likely_matches=[]\n",
    "likely_matches= [x for x in matches if x[2]>90]\n",
    "len(likely_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This piece is to recreate the actual records in the file with an identifier key, the reason is that we could technically have more than one hit per each duplicate found (if the file has duplicates itself)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_list=[]\n",
    "for x in likely_matches:\n",
    "    key1=df_long.loc[df_long['fuzzy'] == x[0]]['member_id'].tolist()\n",
    "    key2=df_short.loc[df_short['fuzzy'] == x[1]]['member_id'].tolist()\n",
    "    output_list.append([x[0], x[1], x[2], key1, key2])\n",
    "output= pd.DataFrame(output_list, columns=['primary','secondary','perc_match','key_primary','key_secondary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>primary</th>\n",
       "      <th>secondary</th>\n",
       "      <th>perc_match</th>\n",
       "      <th>key_primary</th>\n",
       "      <th>key_secondary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>james martin</td>\n",
       "      <td>james martin</td>\n",
       "      <td>100</td>\n",
       "      <td>[7957927902845]</td>\n",
       "      <td>[7488960271747]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>christopher brown</td>\n",
       "      <td>christopher brown</td>\n",
       "      <td>100</td>\n",
       "      <td>[502887664677]</td>\n",
       "      <td>[1970815105834]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>daniel gonzales</td>\n",
       "      <td>daniel gonzales</td>\n",
       "      <td>100</td>\n",
       "      <td>[7993593208263]</td>\n",
       "      <td>[140312516230]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>katherine garcia</td>\n",
       "      <td>garcia katherine h</td>\n",
       "      <td>94</td>\n",
       "      <td>[7876034075542]</td>\n",
       "      <td>[9706101060165]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>joe smith</td>\n",
       "      <td>smith jose</td>\n",
       "      <td>95</td>\n",
       "      <td>[3880509524088]</td>\n",
       "      <td>[4617626102085]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>andrew hernandez</td>\n",
       "      <td>andrew hernandez</td>\n",
       "      <td>100</td>\n",
       "      <td>[2340862914892]</td>\n",
       "      <td>[358339665507]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>christina taylor</td>\n",
       "      <td>christian taylor</td>\n",
       "      <td>94</td>\n",
       "      <td>[648337018130]</td>\n",
       "      <td>[1399534976844]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>katherine smith</td>\n",
       "      <td>katherine smith</td>\n",
       "      <td>100</td>\n",
       "      <td>[8986386973416]</td>\n",
       "      <td>[7563384088130]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>michael taylor</td>\n",
       "      <td>michael taylor</td>\n",
       "      <td>100</td>\n",
       "      <td>[2836812545211]</td>\n",
       "      <td>[5847830015911]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>mary hernandez</td>\n",
       "      <td>mary hernandez</td>\n",
       "      <td>100</td>\n",
       "      <td>[6492089346744]</td>\n",
       "      <td>[7534575761576]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>gary velazquez</td>\n",
       "      <td>vazquez gary</td>\n",
       "      <td>92</td>\n",
       "      <td>[9757549417066]</td>\n",
       "      <td>[8677112089685]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>jessica alvarez</td>\n",
       "      <td>jessica alvarez</td>\n",
       "      <td>100</td>\n",
       "      <td>[9790621226361]</td>\n",
       "      <td>[9250994501583]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>david williams</td>\n",
       "      <td>david williams</td>\n",
       "      <td>100</td>\n",
       "      <td>[3922931562549]</td>\n",
       "      <td>[229720631152]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>george collins</td>\n",
       "      <td>george collins</td>\n",
       "      <td>100</td>\n",
       "      <td>[5179019038436]</td>\n",
       "      <td>[1306491549530]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>jamie owens</td>\n",
       "      <td>james owens</td>\n",
       "      <td>91</td>\n",
       "      <td>[2377196554883]</td>\n",
       "      <td>[410879923414]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>joseph smith</td>\n",
       "      <td>smith jose</td>\n",
       "      <td>91</td>\n",
       "      <td>[2828385250821, 5400110077162]</td>\n",
       "      <td>[4617626102085]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>henry martinez</td>\n",
       "      <td>henry martin</td>\n",
       "      <td>92</td>\n",
       "      <td>[6982911494472]</td>\n",
       "      <td>[9971387465151]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>olivia smith</td>\n",
       "      <td>smith olivia</td>\n",
       "      <td>100</td>\n",
       "      <td>[1067119077629]</td>\n",
       "      <td>[4358287106424]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>andrew smith</td>\n",
       "      <td>andrea smith</td>\n",
       "      <td>92</td>\n",
       "      <td>[1989665637228]</td>\n",
       "      <td>[1896374821001]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>vasquez robert</td>\n",
       "      <td>robert velasquez</td>\n",
       "      <td>93</td>\n",
       "      <td>[9237091857812]</td>\n",
       "      <td>[4936975106842]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              primary           secondary  perc_match  \\\n",
       "0        james martin        james martin         100   \n",
       "1   christopher brown   christopher brown         100   \n",
       "2     daniel gonzales     daniel gonzales         100   \n",
       "3    katherine garcia  garcia katherine h          94   \n",
       "4           joe smith          smith jose          95   \n",
       "5    andrew hernandez    andrew hernandez         100   \n",
       "6    christina taylor    christian taylor          94   \n",
       "7     katherine smith     katherine smith         100   \n",
       "8      michael taylor      michael taylor         100   \n",
       "9      mary hernandez      mary hernandez         100   \n",
       "10     gary velazquez        vazquez gary          92   \n",
       "11    jessica alvarez     jessica alvarez         100   \n",
       "12     david williams      david williams         100   \n",
       "13     george collins      george collins         100   \n",
       "14        jamie owens         james owens          91   \n",
       "15       joseph smith          smith jose          91   \n",
       "16     henry martinez        henry martin          92   \n",
       "17       olivia smith        smith olivia         100   \n",
       "18       andrew smith        andrea smith          92   \n",
       "19     vasquez robert    robert velasquez          93   \n",
       "\n",
       "                       key_primary    key_secondary  \n",
       "0                  [7957927902845]  [7488960271747]  \n",
       "1                   [502887664677]  [1970815105834]  \n",
       "2                  [7993593208263]   [140312516230]  \n",
       "3                  [7876034075542]  [9706101060165]  \n",
       "4                  [3880509524088]  [4617626102085]  \n",
       "5                  [2340862914892]   [358339665507]  \n",
       "6                   [648337018130]  [1399534976844]  \n",
       "7                  [8986386973416]  [7563384088130]  \n",
       "8                  [2836812545211]  [5847830015911]  \n",
       "9                  [6492089346744]  [7534575761576]  \n",
       "10                 [9757549417066]  [8677112089685]  \n",
       "11                 [9790621226361]  [9250994501583]  \n",
       "12                 [3922931562549]   [229720631152]  \n",
       "13                 [5179019038436]  [1306491549530]  \n",
       "14                 [2377196554883]   [410879923414]  \n",
       "15  [2828385250821, 5400110077162]  [4617626102085]  \n",
       "16                 [6982911494472]  [9971387465151]  \n",
       "17                 [1067119077629]  [4358287106424]  \n",
       "18                 [1989665637228]  [1896374821001]  \n",
       "19                 [9237091857812]  [4936975106842]  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>address</th>\n",
       "      <th>city</th>\n",
       "      <th>country</th>\n",
       "      <th>dob</th>\n",
       "      <th>first_name</th>\n",
       "      <th>full_address</th>\n",
       "      <th>full_name</th>\n",
       "      <th>gender</th>\n",
       "      <th>last_name</th>\n",
       "      <th>member_id</th>\n",
       "      <th>middle_name</th>\n",
       "      <th>suffix</th>\n",
       "      <th>title</th>\n",
       "      <th>fuzzy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1216</th>\n",
       "      <td>1216</td>\n",
       "      <td>209 Richard Glens</td>\n",
       "      <td>Smithborough</td>\n",
       "      <td>Peru</td>\n",
       "      <td>2015-04-04</td>\n",
       "      <td>Christina</td>\n",
       "      <td>209 Richard Glens, Smithborough, Peru</td>\n",
       "      <td>Christina Taylor</td>\n",
       "      <td>F</td>\n",
       "      <td>Taylor</td>\n",
       "      <td>648337018130</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mrs.</td>\n",
       "      <td>christina taylor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0            address          city country        dob  \\\n",
       "1216        1216  209 Richard Glens  Smithborough    Peru 2015-04-04   \n",
       "\n",
       "     first_name                           full_address         full_name  \\\n",
       "1216  Christina  209 Richard Glens, Smithborough, Peru  Christina Taylor   \n",
       "\n",
       "     gender last_name     member_id middle_name suffix title             fuzzy  \n",
       "1216      F    Taylor  648337018130         NaN    NaN  Mrs.  christina taylor  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_long[df_long['member_id']==output.iloc[6][3][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>address</th>\n",
       "      <th>city</th>\n",
       "      <th>country</th>\n",
       "      <th>dob</th>\n",
       "      <th>first_name</th>\n",
       "      <th>full_address</th>\n",
       "      <th>full_name</th>\n",
       "      <th>gender</th>\n",
       "      <th>last_name</th>\n",
       "      <th>member_id</th>\n",
       "      <th>middle_name</th>\n",
       "      <th>suffix</th>\n",
       "      <th>title</th>\n",
       "      <th>fuzzy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>205</td>\n",
       "      <td>335 Becker Falls Suite 886</td>\n",
       "      <td>Lake Willieton</td>\n",
       "      <td>Belarus</td>\n",
       "      <td>1942-02-13</td>\n",
       "      <td>Christian</td>\n",
       "      <td>335 Becker Falls Suite 886, Lake Willieton, Be...</td>\n",
       "      <td>Christian Taylor</td>\n",
       "      <td>M</td>\n",
       "      <td>Taylor</td>\n",
       "      <td>1399534976844</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mr.</td>\n",
       "      <td>christian taylor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                     address            city  country  \\\n",
       "205         205  335 Becker Falls Suite 886  Lake Willieton  Belarus   \n",
       "\n",
       "           dob first_name                                       full_address  \\\n",
       "205 1942-02-13  Christian  335 Becker Falls Suite 886, Lake Willieton, Be...   \n",
       "\n",
       "            full_name gender last_name      member_id middle_name suffix  \\\n",
       "205  Christian Taylor      M    Taylor  1399534976844         NaN    NaN   \n",
       "\n",
       "    title             fuzzy  \n",
       "205   Mr.  christian taylor  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_short[df_short['member_id']==output.iloc[6][4][0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for fuzzy duplicates within one file\n",
    "\n",
    "Because we are comparing against itself, the main loop will grow exponentially with the size of the file, very soon you will be looking at millions of iterations. Think carefully how to reduce the dataset to something relevant and manageable.\n",
    "\n",
    "There are alternative methods for large datasets that involve tokenising parts of the texts and scale better possibly reducing accuracy . These are detailed in another notebook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This method below uses a more manual approach to select the combinations of items in this case we can retrieve as many similar items meet the criteria based on the similarity score and not just the top 1 or 2 etc.\n",
    "We need to import the itertools library for its \"combinations\" function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 15)\n",
      "2000\n",
      "1999000 combinations to compute. Careful if this number is large, it may take a long time\n"
     ]
    }
   ],
   "source": [
    "list_to_check= create_fuzzy_column(df_long,\"full_name\")\n",
    "print(df_long.shape)\n",
    "print(len(list_to_check))\n",
    "\n",
    "item_combinations=list(combinations(list_to_check, 2))\n",
    "print(len(item_combinations), \"combinations to compute. Careful if this number is large, it may take a long time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34, 15)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dupes=df_long[df_long['fuzzy'].duplicated(keep=False)].sort_values(by=\"fuzzy\", ascending=False)\n",
    "df_dupes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>address</th>\n",
       "      <th>city</th>\n",
       "      <th>country</th>\n",
       "      <th>dob</th>\n",
       "      <th>first_name</th>\n",
       "      <th>full_address</th>\n",
       "      <th>full_name</th>\n",
       "      <th>gender</th>\n",
       "      <th>last_name</th>\n",
       "      <th>member_id</th>\n",
       "      <th>middle_name</th>\n",
       "      <th>suffix</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fuzzy</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>amanda thompson</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joseph cole</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>michael thomas</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>michael sullivan</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>michael hunt</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>linda brown</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lauren smith</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joseph smith</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>john miller</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brian williams</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>john brown</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jacqueline hernandez</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heather taylor</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>david smith</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>danielle williams</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>christopher moore</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thomas jones</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Unnamed: 0  address  city  country  dob  first_name  \\\n",
       "fuzzy                                                                       \n",
       "amanda thompson                2        2     2        2    2           2   \n",
       "joseph cole                    2        2     2        2    2           2   \n",
       "michael thomas                 2        2     2        2    2           2   \n",
       "michael sullivan               2        2     2        2    2           2   \n",
       "michael hunt                   2        2     2        2    2           2   \n",
       "linda brown                    2        2     2        2    2           2   \n",
       "lauren smith                   2        2     2        2    2           2   \n",
       "joseph smith                   2        2     2        2    2           2   \n",
       "john miller                    2        2     2        2    2           2   \n",
       "brian williams                 2        2     2        2    2           2   \n",
       "john brown                     2        2     2        2    2           2   \n",
       "jacqueline hernandez           2        2     2        2    2           2   \n",
       "heather taylor                 2        2     2        2    2           2   \n",
       "david smith                    2        2     2        2    2           2   \n",
       "danielle williams              2        2     2        2    2           2   \n",
       "christopher moore              2        2     2        2    2           2   \n",
       "thomas jones                   2        2     2        2    2           2   \n",
       "\n",
       "                      full_address  full_name  gender  last_name  member_id  \\\n",
       "fuzzy                                                                         \n",
       "amanda thompson                  2          2       2          2          2   \n",
       "joseph cole                      2          2       2          2          2   \n",
       "michael thomas                   2          2       2          2          2   \n",
       "michael sullivan                 2          2       2          2          2   \n",
       "michael hunt                     2          2       2          2          2   \n",
       "linda brown                      2          2       2          2          2   \n",
       "lauren smith                     2          2       2          2          2   \n",
       "joseph smith                     2          2       2          2          2   \n",
       "john miller                      2          2       2          2          2   \n",
       "brian williams                   2          2       2          2          2   \n",
       "john brown                       2          2       2          2          2   \n",
       "jacqueline hernandez             2          2       2          2          2   \n",
       "heather taylor                   2          2       2          2          2   \n",
       "david smith                      2          2       2          2          2   \n",
       "danielle williams                2          2       2          2          2   \n",
       "christopher moore                2          2       2          2          2   \n",
       "thomas jones                     2          2       2          2          2   \n",
       "\n",
       "                      middle_name  suffix  title  \n",
       "fuzzy                                             \n",
       "amanda thompson                 0       0      2  \n",
       "joseph cole                     0       0      2  \n",
       "michael thomas                  0       0      2  \n",
       "michael sullivan                0       0      2  \n",
       "michael hunt                    0       0      2  \n",
       "linda brown                     0       0      2  \n",
       "lauren smith                    0       0      2  \n",
       "joseph smith                    0       0      2  \n",
       "john miller                     0       0      2  \n",
       "brian williams                  0       0      2  \n",
       "john brown                      0       0      2  \n",
       "jacqueline hernandez            0       0      2  \n",
       "heather taylor                  0       0      2  \n",
       "david smith                     0       0      2  \n",
       "danielle williams               0       0      2  \n",
       "christopher moore               0       0      2  \n",
       "thomas jones                    0       0      2  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dupes.groupby(\"fuzzy\").count().sort_values(by=\"full_name\",ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = []\n",
    "for x in item_combinations:\n",
    "    s = fuzz.token_sort_ratio(x[0],x[1])\n",
    "    if s>60:\n",
    "        matches.append([x[0], x[1], s]) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35, 3)\n"
     ]
    }
   ],
   "source": [
    "df_matches=pd.DataFrame(matches, columns=['x1','x2','score'])\n",
    "likely_matches = df_matches[df_matches.score > 90]\n",
    "print(likely_matches.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 3)\n"
     ]
    }
   ],
   "source": [
    "likely_matches_deduped=likely_matches.drop_duplicates() #we could have exact duplicates in the original file\n",
    "print(likely_matches_deduped.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we still need to go back to the original dataset and find the matching groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_list_fat=[]\n",
    "output_list_thin=[]\n",
    "match_group=1\n",
    "for index,row in likely_matches.iterrows():\n",
    "    key1=df_long.loc[df_long['fuzzy'] == row['x1']]['member_id'].tolist()\n",
    "    if row['x1']==row['x2']:\n",
    "        key2=[] #it is an self match so we count it only once\n",
    "    else:\n",
    "        key2=df_long.loc[df_long['fuzzy'] == row['x2']]['member_id'].tolist()\n",
    "    key_conso=key1+key2\n",
    "    for k in key_conso:\n",
    "        v=df_long[df_long['member_id']==k]['full_name'].to_list()[0] #optional we get the original value\n",
    "        #we could use indexes for extra performance but anyway this routine cannot be run in huge tables\n",
    "        #as the combination would quickly get to tens of millions to check.\n",
    "        output_list_thin.append([k,match_group,row[2],v])\n",
    "    output_list_fat.append([row[0], row[1], row[2], key1, key2, len(key_conso),match_group])\n",
    "    match_group +=1 \n",
    "    \n",
    "output= pd.DataFrame(output_list_fat, columns=['x1','x2','score','k1','k2','countkeys','match_group']).sort_values(by=['score'],ascending=False)\n",
    "output_thin=pd.DataFrame(output_list_thin,columns=['id','match_group','score','value']).sort_values(by=['score'],ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>score</th>\n",
       "      <th>k1</th>\n",
       "      <th>k2</th>\n",
       "      <th>countkeys</th>\n",
       "      <th>match_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>christopher moore</td>\n",
       "      <td>christopher moore</td>\n",
       "      <td>100</td>\n",
       "      <td>[1360800895169, 7072377543230]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>thomas jones</td>\n",
       "      <td>thomas jones</td>\n",
       "      <td>100</td>\n",
       "      <td>[3175587312222, 7249915988253]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>linda brown</td>\n",
       "      <td>linda brown</td>\n",
       "      <td>100</td>\n",
       "      <td>[3213723680900, 184447183004]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>danielle williams</td>\n",
       "      <td>danielle williams</td>\n",
       "      <td>100</td>\n",
       "      <td>[8127440448529, 952131193062]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>john miller</td>\n",
       "      <td>john miller</td>\n",
       "      <td>100</td>\n",
       "      <td>[6774632622452, 6261795470414]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   x1                 x2  score  \\\n",
       "0   christopher moore  christopher moore    100   \n",
       "23       thomas jones       thomas jones    100   \n",
       "20        linda brown        linda brown    100   \n",
       "24  danielle williams  danielle williams    100   \n",
       "18        john miller        john miller    100   \n",
       "\n",
       "                                k1  k2  countkeys  match_group  \n",
       "0   [1360800895169, 7072377543230]  []          2            1  \n",
       "23  [3175587312222, 7249915988253]  []          2           24  \n",
       "20   [3213723680900, 184447183004]  []          2           21  \n",
       "24   [8127440448529, 952131193062]  []          2           25  \n",
       "18  [6774632622452, 6261795470414]  []          2           19  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>match_group</th>\n",
       "      <th>score</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1360800895169</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>Christopher Moore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7072377543230</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>Christopher Moore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>825827228877</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>Michael Sullivan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9520235523057</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>Michael Sullivan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7039363481297</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>Amanda Thompson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>231280510563</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>Amanda Thompson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5433696913272</td>\n",
       "      <td>6</td>\n",
       "      <td>100</td>\n",
       "      <td>Taylor, Billy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>7794666749030</td>\n",
       "      <td>6</td>\n",
       "      <td>100</td>\n",
       "      <td>Billy Taylor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5400110077162</td>\n",
       "      <td>9</td>\n",
       "      <td>100</td>\n",
       "      <td>Joseph Smith</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2828385250821</td>\n",
       "      <td>9</td>\n",
       "      <td>100</td>\n",
       "      <td>Joseph Smith</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>9432861001628</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>Heather Taylor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>8844950381619</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>Heather Taylor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>7537589811664</td>\n",
       "      <td>11</td>\n",
       "      <td>100</td>\n",
       "      <td>Michael Thomas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3747334520428</td>\n",
       "      <td>11</td>\n",
       "      <td>100</td>\n",
       "      <td>Michael Thomas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>7506296213450</td>\n",
       "      <td>12</td>\n",
       "      <td>100</td>\n",
       "      <td>John Brown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>8673512181554</td>\n",
       "      <td>12</td>\n",
       "      <td>100</td>\n",
       "      <td>John Brown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>9001733828207</td>\n",
       "      <td>13</td>\n",
       "      <td>100</td>\n",
       "      <td>Jacqueline Hernandez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3916487516460</td>\n",
       "      <td>13</td>\n",
       "      <td>100</td>\n",
       "      <td>Jacqueline Hernandez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2118643743364</td>\n",
       "      <td>14</td>\n",
       "      <td>100</td>\n",
       "      <td>Anthony Lewis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>9294051750262</td>\n",
       "      <td>14</td>\n",
       "      <td>100</td>\n",
       "      <td>Lewis, Anthony</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>8914835563291</td>\n",
       "      <td>16</td>\n",
       "      <td>100</td>\n",
       "      <td>Diaz, Kevin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1445917465778</td>\n",
       "      <td>16</td>\n",
       "      <td>100</td>\n",
       "      <td>Kevin Diaz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>6774632622452</td>\n",
       "      <td>19</td>\n",
       "      <td>100</td>\n",
       "      <td>John Miller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>6261795470414</td>\n",
       "      <td>19</td>\n",
       "      <td>100</td>\n",
       "      <td>John Miller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>184447183004</td>\n",
       "      <td>21</td>\n",
       "      <td>100</td>\n",
       "      <td>Linda Brown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>3213723680900</td>\n",
       "      <td>21</td>\n",
       "      <td>100</td>\n",
       "      <td>Linda Brown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>6332467549384</td>\n",
       "      <td>22</td>\n",
       "      <td>100</td>\n",
       "      <td>David Smith</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>3054578050438</td>\n",
       "      <td>22</td>\n",
       "      <td>100</td>\n",
       "      <td>David Smith</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>3175587312222</td>\n",
       "      <td>24</td>\n",
       "      <td>100</td>\n",
       "      <td>Thomas Jones</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>7249915988253</td>\n",
       "      <td>24</td>\n",
       "      <td>100</td>\n",
       "      <td>Thomas Jones</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>8127440448529</td>\n",
       "      <td>25</td>\n",
       "      <td>100</td>\n",
       "      <td>Danielle Williams</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>952131193062</td>\n",
       "      <td>25</td>\n",
       "      <td>100</td>\n",
       "      <td>Danielle Williams</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>2447864260207</td>\n",
       "      <td>28</td>\n",
       "      <td>100</td>\n",
       "      <td>Michael Hunt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>3632687508653</td>\n",
       "      <td>28</td>\n",
       "      <td>100</td>\n",
       "      <td>Michael Hunt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>8950577347478</td>\n",
       "      <td>29</td>\n",
       "      <td>100</td>\n",
       "      <td>Lauren Smith</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>8579400177275</td>\n",
       "      <td>29</td>\n",
       "      <td>100</td>\n",
       "      <td>Lauren Smith</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>55177040885</td>\n",
       "      <td>30</td>\n",
       "      <td>100</td>\n",
       "      <td>Brian Williams</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>406297274061</td>\n",
       "      <td>30</td>\n",
       "      <td>100</td>\n",
       "      <td>Brian Williams</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>7345449199476</td>\n",
       "      <td>34</td>\n",
       "      <td>100</td>\n",
       "      <td>Joseph Cole</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>4716904989414</td>\n",
       "      <td>34</td>\n",
       "      <td>100</td>\n",
       "      <td>Joseph Cole</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               id  match_group  score                 value\n",
       "0   1360800895169            1    100     Christopher Moore\n",
       "1   7072377543230            1    100     Christopher Moore\n",
       "4    825827228877            3    100      Michael Sullivan\n",
       "5   9520235523057            3    100      Michael Sullivan\n",
       "9   7039363481297            5    100       Amanda Thompson\n",
       "10   231280510563            5    100       Amanda Thompson\n",
       "12  5433696913272            6    100         Taylor, Billy\n",
       "11  7794666749030            6    100          Billy Taylor\n",
       "18  5400110077162            9    100          Joseph Smith\n",
       "17  2828385250821            9    100          Joseph Smith\n",
       "19  9432861001628           10    100        Heather Taylor\n",
       "20  8844950381619           10    100        Heather Taylor\n",
       "22  7537589811664           11    100        Michael Thomas\n",
       "21  3747334520428           11    100        Michael Thomas\n",
       "24  7506296213450           12    100            John Brown\n",
       "23  8673512181554           12    100            John Brown\n",
       "26  9001733828207           13    100  Jacqueline Hernandez\n",
       "25  3916487516460           13    100  Jacqueline Hernandez\n",
       "28  2118643743364           14    100         Anthony Lewis\n",
       "27  9294051750262           14    100        Lewis, Anthony\n",
       "31  8914835563291           16    100           Diaz, Kevin\n",
       "32  1445917465778           16    100            Kevin Diaz\n",
       "39  6774632622452           19    100           John Miller\n",
       "40  6261795470414           19    100           John Miller\n",
       "45   184447183004           21    100           Linda Brown\n",
       "44  3213723680900           21    100           Linda Brown\n",
       "46  6332467549384           22    100           David Smith\n",
       "47  3054578050438           22    100           David Smith\n",
       "50  3175587312222           24    100          Thomas Jones\n",
       "51  7249915988253           24    100          Thomas Jones\n",
       "52  8127440448529           25    100     Danielle Williams\n",
       "53   952131193062           25    100     Danielle Williams\n",
       "59  2447864260207           28    100          Michael Hunt\n",
       "60  3632687508653           28    100          Michael Hunt\n",
       "62  8950577347478           29    100          Lauren Smith\n",
       "61  8579400177275           29    100          Lauren Smith\n",
       "64    55177040885           30    100        Brian Williams\n",
       "63   406297274061           30    100        Brian Williams\n",
       "73  7345449199476           34    100           Joseph Cole\n",
       "72  4716904989414           34    100           Joseph Cole"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_thin.iloc[0:40].sort_values(by='match_group')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
